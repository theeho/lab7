---
title: "lab7"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(knitr)
library(broom)
library(nnet)
library(broom)
library(pROC)
library(plotROC) 
library(arm)

```

Exercise 1: 

```{r}
data <- read.csv("data.csv")
glimpse(data)
```
```{r}

data_lv <- data %>% mutate(target = as.factor(target), key = as.factor(ifelse(key == 2, "D", ifelse(key == 3, "D#", "Other"))))

glimpse(data_lv)
```


```{r}
p <- ggplot(data = data_lv, aes(x = key, fill = target)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion", 
       title = "target vs key") +
  coord_flip()
p
```
It appears that D and Other both have around an equal proportion of 0 and 1 targets with slightly more 1 target values. Key D# has a little over twice as many 0 target values than 1 target values. 

Exercise 2: 
```{r}
target_m_red <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence, 
              data = data_lv, family = binomial)
tidy(target_m_red, conf.int = TRUE, exponentiate = FALSE) %>% 
  kable(format = "markdown", digits = 3)
```
Exercise 3: 
```{r}
target_m_full <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence + key, 
              data = data_lv, family = binomial)
```

```{r}

anova(target_m_red, target_m_full, test = "Chisq")
```

There is evidence to suggest that the key is a significant predictor because we have a low p value of .001258. Therefor based on the test, we should add key to the model. 


Exercise 4:

```{r}
model <- target_m_full 
tidy(model, conf.int = TRUE, exponentiate = FALSE) %>% 
  kable(format = "markdown", digits = 3)
```
The keyD# coefficent tells us how the log odds of the target = 1 will change if our track is in the key of D#. 

Exercise 5:

```{r}
m_aug <- augment(model, type.predict = "response", 
                      type.residuals = "deviance")


```
Exercise 6:
```{r}
arm::binnedplot(x = m_aug$.fitted, y = m_aug$.resid,
                xlab = "Predicted Probabilities", 
                main = "Binned Residual vs. Predicted Values", 
                col.int = FALSE)
```
Exercise 7:
```{r}
arm::binnedplot(x = m_aug$instrumentalness, 
                y = m_aug$.resid, 
                col.int = FALSE,
                xlab = "instrumentalness", 
                main = "Binned Residual vs. instrumentalness")
```
Exercise 8: 
```{r}
m_aug %>%
  group_by(key) %>%
  summarise(mean_resid = mean(.resid))
```

Exercise 9: 
Both the key and instrumental residuals do not show evidence of constant variance. There also seems to be a partern ascociated with the average residual vs probability plot. Based on this, linearity assumption is not statsfied. 

Exercise 10:
```{r}
(roc_curve <- ggplot(m_aug, 
                     aes(d = as.numeric(target) - 1, 
                         m = .fitted)) +
  geom_roc(n.cuts = 5, labelround = 3) + 
  geom_abline(intercept = 0) + 
  labs(x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)") )
calc_auc(roc_curve)$AUC

```
Exercise 11:
The model appears to be somewhat effective. However, we would like the AOC to be higher. 

Exercise 12:
```{r}
threshold <- .493
```
I chose this threshold because it is closest to the top left corner of the plot. That is, maximum true positive rate and minimum false positive rate. 

Exercise 13:

```{r}
m_aug %>%
  mutate(predict_target = if_else(.fitted > threshold, "1", "0")) %>%
  group_by(target, predict_target) %>%
  summarise(n = n()) %>%
  kable(format="markdown")
```
Exercise 14:
The proprotion of true positives is 680/(680+340) = 2/3
The proportion of false positives is 326/(326+671)= .32
The misclassification rate is (340+326)/(671+326+340+680) = .33 




